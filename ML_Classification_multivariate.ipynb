{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNABIu6BMGX/glPoRaRLhNO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajkc-dev/research-deeplearning-tensorflow-keras/blob/main/ML_Classification_multivariate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We start by importing the modules that we will need. In the previous example, we normalized\n",
        "our data using the DataFrame operations. In this example, we will make use of the Keras\n",
        "Normalization layer. **The Normalization layer shifts the data to a zero mean and one standard**\n",
        "deviation. Also, since we have more than one independent variable, we will use Seaborn to\n",
        "visualize the relationship between different variables:"
      ],
      "metadata": {
        "id": "Sn8Y-s1wkxS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8hqAoG7QjU6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow.keras as K\n",
        "from tensorflow.keras.layers import Dense, Normalization\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Let us first download the data from the UCI ML repo."
      ],
      "metadata": {
        "id": "WLrybUdukx36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
        "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower',\n",
        "'weight', 'acceleration', 'model_year', 'origin']\n",
        "data = pd.read_csv(url, names=column_names, na_values='?', comment='\\t',\n",
        "sep=' ', skipinitialspace=True)"
      ],
      "metadata": {
        "id": "ulFSwCMEVe-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The data consists of eight features: mpg, cylinders, displacement, horsepower, weight,\n",
        "acceleration, model year, and origin. Though the origin of the vehicle can also affect the fuel\n",
        "efficiency “mpg” (miles per gallon), we use only seven features to predict the mpg value. Also,\n",
        "we drop any rows with NaN values:"
      ],
      "metadata": {
        "id": "h0IRt2R8kyZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop('origin', axis=1)\n",
        "print(data.isna().sum())\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "QK7tKNHUVxKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The error message `TypeError: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given` indicates that you are passing too many arguments to the `drop()` method of your Pandas DataFrame.\n",
        "In older versions of Pandas, you could specify the `axis` parameter as `0` for rows and `1` for columns directly as a positional argument. However, in more recent versions, the `axis` parameter should be explicitly named.\n",
        "To fix this, you need to specify `axis=1` when dropping a column.\n",
        "Here's how to correct your code:\n",
        "\n",
        "```python\n",
        "data = data.drop('origin', axis=1)\n",
        "print(data.isna().sum())\n",
        "data = data.dropna()\n",
        "```\n",
        "\n",
        "> By adding `axis=1`, you are explicitly telling Pandas to drop the column labeled 'origin'."
      ],
      "metadata": {
        "id": "3Up-U6PrnSX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. We divide the dataset into training and test datasets. Here, we are keeping 80% of the 392\n",
        "datapoints as training data and 20% as test dataset:"
      ],
      "metadata": {
        "id": "_Qm_8dk2kzmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = data.sample(frac=0.8, random_state=0)\n",
        "test_dataset = data.drop(train_dataset.index)"
      ],
      "metadata": {
        "id": "UrbcW23YV13m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Next, we use Seaborn’s pairplot to visualize the relationship between the different variables:"
      ],
      "metadata": {
        "id": "AVOoIyKwk0MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(train_dataset[['mpg', 'cylinders',\n",
        "'displacement','horsepower', 'weight', 'acceleration', 'model_year']],\n",
        "diag_kind='kde')"
      ],
      "metadata": {
        "id": "s-FOSZaxV6Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. We can see that mpg (fuel efficiency) has dependencies on all the other variables, and the\n",
        "dependency relationship is non-linear, as none of the curves are linear:"
      ],
      "metadata": {
        "id": "nLJFpXEGk00f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. For convenience, we also separate the variables into input variables and the label that we\n",
        "want to predict:"
      ],
      "metadata": {
        "id": "9e5jJveglzR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "train_labels = train_features.pop('mpg')\n",
        "test_labels = test_features.pop('mpg')"
      ],
      "metadata": {
        "id": "8Pu97sg3WQN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Now, we use the Normalization layer of Keras to normalize our data. Note that while we\n",
        "normalized our inputs to a value with mean 0 and standard deviation 1, the output prediction\n",
        "'mpg' remains as it is:"
      ],
      "metadata": {
        "id": "o6yqNN2glyJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize\n",
        "data_normalizer = Normalization(axis=1)\n",
        "data_normalizer.adapt(np.array(train_features))"
      ],
      "metadata": {
        "id": "b-LfTKoyWVHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. We build our model. The model has two hidden layers, with 64 and 32 neurons, respectively.\n",
        "For the hidden layers, we have used Rectified Linear Unit (ReLU) as our activation function;\n",
        "this should help in approximating the non-linear relation between fuel efficiency and the rest\n",
        "of the variables:"
      ],
      "metadata": {
        "id": "Rl6WoSEKk2fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = K.Sequential([\n",
        "data_normalizer,\n",
        "Dense(64, activation='relu'),\n",
        "Dense(32, activation='relu'),\n",
        "Dense(1, activation=None)\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7_7DNffVWadm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Earlier, we used stochastic gradient as the optimizer; this time, we try the Adam optimizer\n",
        "(see Chapter 1, Neural Network Foundations with TF, for more details). The loss function for the\n",
        "regression we chose is the mean squared error again:"
      ],
      "metadata": {
        "id": "KrKHI1ank332"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "rsxQkka4jXRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Next, we train the model for 100 epochs:"
      ],
      "metadata": {
        "id": "bKpa7jelk17D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x=train_features,y=train_labels, epochs=100,\n",
        "verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "pOYaMzsPjZFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Cool, now that the model is trained, we can check if our model is overfitted, underfitted, or\n",
        "properly fitted by plotting the loss curve. Both validation loss and training loss are near each\n",
        "other as we increase the training epochs; this suggests that our model is properly trained:"
      ],
      "metadata": {
        "id": "FnyQKPhck4Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error [MPG]')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "BjMfsyuLjeI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Let us finally compare the predicted fuel efficiency and the true fuel efficiency on the test dataset.\n",
        "Remember that the model has not seen a test dataset ever, thus this prediction is from the\n",
        "model’s ability to generalize the relationship between inputs and fuel efficiency. If the model\n",
        "has learned the relationship well, the two should form a linear relationship:"
      ],
      "metadata": {
        "id": "MSGWg9iNk430"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(test_features).flatten()\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, y_pred)\n",
        "plt.xlabel('True Values [MPG]')\n",
        "plt.ylabel('Predictions [MPG]')\n",
        "lims = [0, 50]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "plt.plot(lims, lims)"
      ],
      "metadata": {
        "id": "U1I6oGFQjveQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Additionally, we can also plot the error between the predicted and true fuel efficiency:"
      ],
      "metadata": {
        "id": "86GdHPq9k5K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error = y_pred - test_labels\n",
        "plt.hist(error, bins=30)\n",
        "plt.xlabel('Prediction Error [MPG]')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "cVEnZVTtjxq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case we want to make more than one prediction, that is, dealing with a multivariate regression\n",
        "problem, the only change would be that instead of one unit in the last dense layer, we will have as\n",
        "many units as the number of variables to be predicted. Consider, for example, we want to build a\n",
        "model which takes into account a student’s SAT score, attendance, and some family parameters, and\n",
        "wants to predict the GPA score for all four undergraduate years; then we will have the output layer\n",
        "with four units. Now that you are familiar with regression, let us move toward the classification tasks."
      ],
      "metadata": {
        "id": "jzT2kCg0muAx"
      }
    }
  ]
}